{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3UWqNLVoapio"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "%matplotlib inline\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import distributions as D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuqEoTr0apir"
      },
      "source": [
        "Parameter Settings\n",
        "-------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7J1ZdCslapis"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w72CRzGMapiu"
      },
      "source": [
        "MNIST Data Loading\n",
        "-------------------\n",
        "\n",
        "MNIST images show digits from 0-9 in 28x28 grayscale images. We do not center them at 0, because we will be using a binary cross-entropy loss that treats pixel values as probabilities in [0,1]. We create both a training set and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pZhKvaJLapiu"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = MNIST(root='./data/MNIST', download=True, train=False, transform=img_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_iGjhTGy7YSl"
      },
      "outputs": [],
      "source": [
        "class HVAE(nn.Module):\n",
        "  def __init__(self, device):\n",
        "    super(HVAE, self).__init__()\n",
        "\n",
        "    self.device = device\n",
        "    self.c = 16\n",
        "    self.z_dims = 16\n",
        "    self.mu_dims = 4\n",
        "\n",
        "    # Layers for q(z|x):\n",
        "    self.qz_conv1 = nn.Conv2d(in_channels=1, out_channels=self.c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14\n",
        "    self.qz_conv2 = nn.Conv2d(in_channels=self.c, out_channels=self.c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
        "    self.qz_mu = nn.Linear(in_features=self.c*2*7*7, out_features=self.z_dims)\n",
        "    self.qz_pre_sp = nn.Linear(in_features=self.c*2*7*7, out_features=self.z_dims)\n",
        "\n",
        "    # Layers for q(mu|z):\n",
        "    h_dims = self.z_dims // 2\n",
        "    self.qmu_l1 = nn.Linear(in_features=self.z_dims, out_features=h_dims)\n",
        "    # self.qmu_l2 = nn.Linear(in_features=h_dims, out_features=(h_dims//2))\n",
        "    # h_dims = h_dims // 2\n",
        "    self.qmu_mu = nn.Linear(in_features=h_dims, out_features=self.mu_dims)\n",
        "    self.qmu_pre_sp = nn.Linear(in_features=h_dims, out_features=self.mu_dims)\n",
        "\n",
        "    # Layers for p(z|mu):\n",
        "    h_dims = self.mu_dims * 2\n",
        "    self.pz_l1 = nn.Linear(in_features=self.mu_dims, out_features=h_dims)\n",
        "    # self.pz_l2 = nn.Linear(in_features=h_dims, out_features=(h_dims*2))\n",
        "    # h_dims = h_dims * 2\n",
        "    self.pz_mu = nn.Linear(in_features=h_dims, out_features=self.z_dims)\n",
        "    self.pz_pre_sp = nn.Linear(in_features=h_dims, out_features=self.z_dims)\n",
        "\n",
        "    # Layers for p(x|z):\n",
        "    self.px_l1 = nn.Linear(in_features=self.z_dims, out_features=self.c*2*7*7)\n",
        "    self.px_conv1 = nn.ConvTranspose2d(in_channels=self.c*2, out_channels=self.c, kernel_size=4, stride=2, padding=1)\n",
        "    self.px_conv2 = nn.ConvTranspose2d(in_channels=self.c, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "  def q_z(self, x):\n",
        "    h = F.relu(self.qz_conv1(x))\n",
        "    h = F.relu(self.qz_conv2(h))\n",
        "    h = h.view(h.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "    z_mu = self.qz_mu(h)\n",
        "    z_pre_sp = self.qz_pre_sp(h)\n",
        "    z_std = F.softplus(z_pre_sp)\n",
        "    return self.reparameterize(z_mu, z_std), z_mu, z_std\n",
        "\n",
        "  def q_mu(self, z):\n",
        "    h = F.relu(self.qmu_l1(z))\n",
        "    # h = F.relu(self.qmu_l2(h))\n",
        "    mu_mu = self.qmu_mu(h)\n",
        "    mu_pre_sp = self.qmu_pre_sp(h)\n",
        "    mu_std = F.softplus(mu_pre_sp)\n",
        "    return self.reparameterize(mu_mu, mu_std), mu_mu, mu_std\n",
        "\n",
        "  def p_z(self, mu):\n",
        "    h = F.relu(self.pz_l1(mu))\n",
        "    # h = F.relu(self.pz_l2(h))\n",
        "    z_mu = self.pz_mu(h)\n",
        "    z_pre_sp = self.pz_pre_sp(h)\n",
        "    z_std = F.softplus(z_pre_sp)\n",
        "    return self.reparameterize(z_mu, z_std), z_mu, z_std\n",
        "\n",
        "  def p_x(self, z):\n",
        "    h = self.px_l1(z)\n",
        "    h = h.view(h.size(0), self.c*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "    h = F.relu(self.px_conv1(h))\n",
        "    x = torch.sigmoid(self.px_conv2(h)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
        "    return x\n",
        "\n",
        "  def reparameterize(self, mu, std):\n",
        "    eps = Variable(torch.randn(mu.size()))\n",
        "    eps = eps.to(self.device)\n",
        "\n",
        "    return mu + eps * std\n",
        "\n",
        "  def sample_x(self, num=10):\n",
        "    # sample latent vectors from the normal distribution\n",
        "    mu = torch.randn(num, self.mu_dims)\n",
        "    mu = mu.to(self.device)\n",
        "\n",
        "    z_hat, _, _ = self.p_z(mu)\n",
        "    x_prob = self.p_x(z_hat)\n",
        "\n",
        "    return x_prob\n",
        "\n",
        "  def reconstruction(self, x):\n",
        "    z, _, _ = self.q_z(x)\n",
        "    x_prob = self.p_x(z)\n",
        "\n",
        "    return x_prob\n",
        "\n",
        "  def forward(self, x):\n",
        "    z, qz_mu, qz_std = self.q_z(x)\n",
        "    mu, qmu_mu, qmu_std = self.q_mu(z)\n",
        "    _, pz_mu, pz_std = self.p_z(mu)\n",
        "    x_prob = self.p_x(z)\n",
        "\n",
        "    # For likelihood : <log p(x|z)>_q :\n",
        "    elbo = torch.sum(torch.flatten(x.view(-1, 784) * torch.log(x_prob.view(-1, 784) + 1e-8)\n",
        "                                    + (1 - x.view(-1, 784)) * torch.log(1 - x_prob.view(-1, 784) + 1e-8),\n",
        "                                    start_dim=1),\n",
        "                      dim=-1)\n",
        "    \n",
        "    qmu = D.normal.Normal(qmu_mu, qmu_std)\n",
        "    qmu = D.independent.Independent(qmu, 1)\n",
        "    qz = D.normal.Normal(qz_mu, qz_std)\n",
        "    qz = D.independent.Independent(qz, 1)\n",
        "    pz = D.normal.Normal(pz_mu, pz_std)\n",
        "    pz = D.independent.Independent(pz, 1)\n",
        "    pmu = D.normal.Normal(torch.zeros_like(mu), torch.ones_like(mu))\n",
        "    pmu = D.independent.Independent(pmu, 1)\n",
        "    # For : <log p(z|u)>_q\n",
        "    elbo += pz.log_prob(z)\n",
        "\n",
        "    # For : <log p(mu)>_q\n",
        "    elbo += pmu.log_prob(mu)\n",
        "\n",
        "    # For : -<log q(mu|z)>_q\n",
        "    elbo -= qmu.log_prob(mu)\n",
        "\n",
        "    # For : -<log q(z|x)>_q\n",
        "    elbo -= qz.log_prob(z)\n",
        "\n",
        "    return -elbo.mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhx9R8G0YlpK",
        "outputId": "56f9ffe3-a093-44f3-e684-9a91373af951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "hvae = HVAE(device)\n",
        "hvae = hvae.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFTNj2GRZB1U",
        "outputId": "eeee9712-06f5-41ce-ff4c-9537fd06a53c"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=hvae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# set to training mode\n",
        "hvae.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "    \n",
        "    for image_batch, _ in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        image_batch = image_batch.to(device)\n",
        "\n",
        "        loss = hvae(image_batch)\n",
        "        \n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-6d9aJdAHFe"
      },
      "outputs": [],
      "source": [
        "# this is how the VAE parameters can be saved:\n",
        "# torch.save(hvae.state_dict(), './pretrained/hvae.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "nIhbc4gko-Mv",
        "outputId": "40f60124-e8cf-4be4-f3b9-25a0c3e319fe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "\n",
        "import torchvision.utils\n",
        "\n",
        "hvae.eval()\n",
        "\n",
        "# This function takes as an input the images to reconstruct\n",
        "# and the name of the model with which the reconstructions\n",
        "# are performed\n",
        "def to_img(x):\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        images = images.to(device)\n",
        "        images = hvae.reconstruction(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = torchvision.utils.make_grid(images[1:50], 10, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "images, labels = iter(test_dataloader).next()\n",
        "\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(torchvision.utils.make_grid(images[1:50],10,5))\n",
        "plt.show()\n",
        "\n",
        "# Reconstruct and visualise the images using the vae\n",
        "print('VAE reconstruction:')\n",
        "visualise_output(images, hvae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "DxKdXkoZfARp",
        "outputId": "ece99658-6e10-4fb1-d361-3892752f59d2"
      },
      "outputs": [],
      "source": [
        "hvae.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # sample images\n",
        "    img_samples = hvae.sample_x()\n",
        "    img_samples = img_samples.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    show_image(torchvision.utils.make_grid(img_samples,10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6FHAgURxu-W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Hierarchical VAE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
